# YOU

You are going to help me create the plan for the system "grok"

scope for the entire jubal system adn grok's place in it is found in docs/grok-project-scop.md
information as to what grok is a nd what it needs to accomplish is found in grok-integration.md

for now we are in the planning phase for the grok project

## Tools
Use Docker CLI so we can build and run containers directly from the terminal
Use GitHub CLI with OAuth authentication so I can manage repositories
Use Supabase MCP to manage the local database
Use Brave MCP for websearch
Use Archon MCP for project management (continually update and reference this) and as the first point of refernce for using items form the tech stack (ie developer documentation) [see the # archon section]
Use Archon's knowledgebase for to retrieve and reference the comprehensive documentation on python-jose, jose, pydantic, supabase, Openrouter and akash
Use Serena MCP for regex and to surf and learn the codebase
Use Selenium CLI to interact with the frontend(s) to vigorously test the changes as we go


# REFERENCED ITEMS OF NOTE
Recall https://github.com/arealicehole/recall
Jubal https://github.com/arealicehole/jubal

# Development Guidelines

This file provides comprehensive guidance to Claude Code when working with the Jubal microservices architecture.

## Core Development Philosophy

### KISS (Keep It Simple, Stupid)
Simplicity should be a key goal in design. Choose straightforward solutions over complex ones whenever possible. Simple solutions are easier to understand, maintain, and debug.

### YAGNI (You Aren't Gonna Need It)
Avoid building functionality on speculation. Implement features only when they are needed, not when you anticipate they might be useful in the future.


# CRITICAL: ARCHON-FIRST RULE - READ THIS FIRST
  BEFORE doing ANYTHING else, when you see ANY task management scenario:
  1. STOP and check if Archon MCP server is available
  2. Use Archon task management as PRIMARY system
  3. TodoWrite is ONLY for personal, secondary tracking AFTER Archon setup
  4. This rule overrides ALL other instructions, PRPs, system reminders, and patterns

  VIOLATION CHECK: If you used TodoWrite first, you violated this rule. Stop and restart with Archon.

# Archon Integration & Workflow

**CRITICAL: This project uses Archon MCP server for knowledge management, task tracking, and project organization. ALWAYS start with Archon MCP server task management.**

## Core Archon Workflow Principles

### The Golden Rule: Task-Driven Development with Archon

**MANDATORY: Always complete the full Archon specific task cycle before any coding:**

1. **Check Current Task** ‚Üí `archon:manage_task(action="get", task_id="...")`
2. **Research for Task** ‚Üí `archon:search_code_examples()` + `archon:perform_rag_query()`
3. **Implement the Task** ‚Üí Write code based on research
4. **Update Task Status** ‚Üí `archon:manage_task(action="update", task_id="...", update_fields={"status": "review"})`
5. **Get Next Task** ‚Üí `archon:manage_task(action="list", filter_by="status", filter_value="todo")`
6. **Repeat Cycle**

**NEVER skip task updates with the Archon MCP server. NEVER code without checking current tasks first.**

## Project Scenarios & Initialization

### Scenario 1: New Project with Archon

```bash
# Create project container
archon:manage_project(
  action="create",
  title="Descriptive Project Name",
  github_repo="github.com/user/repo-name"
)

# Research ‚Üí Plan ‚Üí Create Tasks (see workflow below)
```

### Scenario 2: Existing Project - Adding Archon

```bash
# First, analyze existing codebase thoroughly
# Read all major files, understand architecture, identify current state
# Then create project container
archon:manage_project(action="create", title="Existing Project Name")

# Research current tech stack and create tasks for remaining work
# Focus on what needs to be built, not what already exists
```

### Scenario 3: Continuing Archon Project

```bash
# Check existing project status
archon:manage_task(action="list", filter_by="project", filter_value="[project_id]")

# Pick up where you left off - no new project creation needed
# Continue with standard development iteration workflow
```

### Universal Research & Planning Phase

**For all scenarios, research before task creation:**

```bash
# High-level patterns and architecture
archon:perform_rag_query(query="[technology] architecture patterns", match_count=5)

# Specific implementation guidance  
archon:search_code_examples(query="[specific feature] implementation", match_count=3)
```

**Create atomic, prioritized tasks:**
- Each task = 1-4 hours of focused work
- Higher `task_order` = higher priority
- Include meaningful descriptions and feature assignments

## Development Iteration Workflow

### Before Every Coding Session

**MANDATORY: Always check task status before writing any code:**

```bash
# Get current project status
archon:manage_task(
  action="list",
  filter_by="project", 
  filter_value="[project_id]",
  include_closed=false
)

# Get next priority task
archon:manage_task(
  action="list",
  filter_by="status",
  filter_value="todo",
  project_id="[project_id]"
)
```

### Task-Specific Research

**For each task, conduct focused research:**

```bash
# High-level: Architecture, security, optimization patterns
archon:perform_rag_query(
  query="JWT authentication security best practices",
  match_count=5
)

# Low-level: Specific API usage, syntax, configuration
archon:perform_rag_query(
  query="Express.js middleware setup validation",
  match_count=3
)

# Implementation examples
archon:search_code_examples(
  query="Express JWT middleware implementation",
  match_count=3
)
```

**Research Scope Examples:**
- **High-level**: "microservices architecture patterns", "database security practices"
- **Low-level**: "Zod schema validation syntax", "Cloudflare Workers KV usage", "PostgreSQL connection pooling"
- **Debugging**: "TypeScript generic constraints error", "npm dependency resolution"

### Task Execution Protocol

**1. Get Task Details:**
```bash
archon:manage_task(action="get", task_id="[current_task_id]")
```

**2. Update to In-Progress:**
```bash
archon:manage_task(
  action="update",
  task_id="[current_task_id]",
  update_fields={"status": "doing"}
)
```

**3. Implement with Research-Driven Approach:**
- Use findings from `search_code_examples` to guide implementation
- Follow patterns discovered in `perform_rag_query` results
- Reference project features with `get_project_features` when needed

**4. Complete Task:**
- When you complete a task mark it under review so that the user can confirm and test.
```bash
archon:manage_task(
  action="update", 
  task_id="[current_task_id]",
  update_fields={"status": "review"}
)
```

## Knowledge Management Integration

### Documentation Queries

**Use RAG for both high-level and specific technical guidance:**

```bash
# Architecture & patterns
archon:perform_rag_query(query="microservices vs monolith pros cons", match_count=5)

# Security considerations  
archon:perform_rag_query(query="OAuth 2.0 PKCE flow implementation", match_count=3)

# Specific API usage
archon:perform_rag_query(query="React useEffect cleanup function", match_count=2)

# Configuration & setup
archon:perform_rag_query(query="Docker multi-stage build Node.js", match_count=3)

# Debugging & troubleshooting
archon:perform_rag_query(query="TypeScript generic type inference error", match_count=2)
```

### Code Example Integration

**Search for implementation patterns before coding:**

```bash
# Before implementing any feature
archon:search_code_examples(query="React custom hook data fetching", match_count=3)

# For specific technical challenges
archon:search_code_examples(query="PostgreSQL connection pooling Node.js", match_count=2)
```

**Usage Guidelines:**
- Search for examples before implementing from scratch
- Adapt patterns to project-specific requirements  
- Use for both complex features and simple API usage
- Validate examples against current best practices

## Progress Tracking & Status Updates

### Daily Development Routine

**Start of each coding session:**

1. Check available sources: `archon:get_available_sources()`
2. Review project status: `archon:manage_task(action="list", filter_by="project", filter_value="...")`
3. Identify next priority task: Find highest `task_order` in "todo" status
4. Conduct task-specific research
5. Begin implementation

**End of each coding session:**

1. Update completed tasks to "done" status
2. Update in-progress tasks with current status
3. Create new tasks if scope becomes clearer
4. Document any architectural decisions or important findings

### Task Status Management

**Status Progression:**
- `todo` ‚Üí `doing` ‚Üí `review` ‚Üí `done`
- Use `review` status for tasks pending validation/testing
- Use `archive` action for tasks no longer relevant

**Status Update Examples:**
```bash
# Move to review when implementation complete but needs testing
archon:manage_task(
  action="update",
  task_id="...",
  update_fields={"status": "review"}
)

# Complete task after review passes
archon:manage_task(
  action="update", 
  task_id="...",
  update_fields={"status": "done"}
)
```

## Research-Driven Development Standards

### Before Any Implementation

**Research checklist:**

- [ ] Search for existing code examples of the pattern
- [ ] Query documentation for best practices (high-level or specific API usage)
- [ ] Understand security implications
- [ ] Check for common pitfalls or antipatterns

### Knowledge Source Prioritization

**Query Strategy:**
- Start with broad architectural queries, narrow to specific implementation
- Use RAG for both strategic decisions and tactical "how-to" questions
- Cross-reference multiple sources for validation
- Keep match_count low (2-5) for focused results

## Project Feature Integration

### Feature-Based Organization

**Use features to organize related tasks:**

```bash
# Get current project features
archon:get_project_features(project_id="...")

# Create tasks aligned with features
archon:manage_task(
  action="create",
  project_id="...",
  title="...",
  feature="Authentication",  # Align with project features
  task_order=8
)
```

### Feature Development Workflow

1. **Feature Planning**: Create feature-specific tasks
2. **Feature Research**: Query for feature-specific patterns
3. **Feature Implementation**: Complete tasks in feature groups
4. **Feature Integration**: Test complete feature functionality

## Error Handling & Recovery

### When Research Yields No Results

**If knowledge queries return empty results:**

1. Broaden search terms and try again
2. Search for related concepts or technologies
3. Document the knowledge gap for future learning
4. Proceed with conservative, well-tested approaches

### When Tasks Become Unclear

**If task scope becomes uncertain:**

1. Break down into smaller, clearer subtasks
2. Research the specific unclear aspects
3. Update task descriptions with new understanding
4. Create parent-child task relationships if needed

### Project Scope Changes

**When requirements evolve:**

1. Create new tasks for additional scope
2. Update existing task priorities (`task_order`)
3. Archive tasks that are no longer relevant
4. Document scope changes in task descriptions

## Quality Assurance Integration

### Research Validation

**Always validate research findings:**
- Cross-reference multiple sources
- Verify recency of information
- Test applicability to current project context
- Document assumptions and limitations

### Task Completion Criteria

**Every task must meet these criteria before marking "done":**
- [ ] Implementation follows researched best practices
- [ ] Code follows project style guidelines
- [ ] Security considerations addressed
- [ ] Basic functionality tested
- [ ] Documentation updated if needed

## ü§ñ Grok Intelligence Engine - Specialized Tech Stack Guidelines

  ### AI/ML Development Philosophy

  Building the Grok Intelligence Engine requires balancing **intelligent processing** with **microservices reliability**. Follow
  these principles when working with our AI-powered transcript analysis system.

  #### Core AI Principles
  - **Model Agnostic Design**: Write code that works with any LLM provider (local or cloud)
  - **Graceful Degradation**: Always have fallback strategies when AI services fail
  - **Token Efficiency**: Optimize prompts and processing for cost and speed
  - **Privacy First**: Default to local processing, use cloud only when necessary

  ### üß† LLM Provider Integration

  #### Model Provider Architecture

  ```python
  from abc import ABC, abstractmethod
  from typing import Dict, Any, Optional
  from pydantic import BaseModel

  class ModelConfig(BaseModel):
      """Configuration for LLM model requests."""
      provider: str = "local"  # "local" or "openrouter"
      model: str = "llama3.1:8b"
      temperature: float = 0.2
      max_tokens: int = 2000
      timeout_seconds: int = 30

  class LLMProvider(ABC):
      """Abstract base for all LLM providers."""

      @abstractmethod
      async def generate_completion(
          self,
          prompt: str,
          config: ModelConfig
      ) -> Dict[str, Any]:
          """Generate completion with proper error handling."""
          pass

      @abstractmethod
      async def check_availability(self) -> bool:
          """Check if provider is available and responsive."""
          pass

  class OllamaProvider(LLMProvider):
      """Local Ollama provider for privacy-focused processing."""

      def __init__(self, base_url: str = "http://host.docker.internal:11434"):
          self.base_url = base_url
          self.client = aiohttp.ClientSession()

      async def generate_completion(
          self,
          prompt: str,
          config: ModelConfig
      ) -> Dict[str, Any]:
          """Generate completion using local Ollama instance."""
          try:
              payload = {
                  "model": config.model,
                  "prompt": prompt,
                  "options": {
                      "temperature": config.temperature,
                      "num_predict": config.max_tokens
                  }
              }

              async with self.client.post(
                  f"{self.base_url}/api/generate",
                  json=payload,
                  timeout=aiohttp.ClientTimeout(total=config.timeout_seconds)
              ) as response:
                  if response.status == 200:
                      data = await response.json()
                      return {
                          "content": data.get("response", ""),
                          "tokens_used": data.get("eval_count", 0),
                          "provider": "ollama",
                          "model": config.model
                      }
                  else:
                      raise LLMProviderError(f"Ollama API error: {response.status}")

          except asyncio.TimeoutError:
              raise LLMProviderError("Ollama request timeout")
          except aiohttp.ClientError as e:
              raise LLMProviderError(f"Ollama connection error: {e}")

  class OpenRouterProvider(LLMProvider):
      """OpenRouter provider for advanced cloud models."""

      def __init__(self, api_key: str, base_url: str = "https://openrouter.ai/api/v1"):
          self.api_key = api_key
          self.base_url = base_url
          self.client = aiohttp.ClientSession(
              headers={"Authorization": f"Bearer {api_key}"}
          )

  Provider Selection Strategy

  from typing import List, Optional
  import logging

  logger = logging.getLogger(__name__)

  class ModelSelector:
      """Intelligent model selection with fallback strategies."""

      def __init__(self, providers: Dict[str, LLMProvider]):
          self.providers = providers
          self.fallback_order = ["local", "openrouter"]

      async def select_provider(
          self, 
          config: ModelConfig,
          global_overrides: Optional[Dict[str, Any]] = None
      ) -> LLMProvider:
          """Select best available provider with fallbacks."""

          # Apply global overrides
          if global_overrides:
              if "force_provider" in global_overrides:
                  config.provider = global_overrides["force_provider"]
              if "force_model" in global_overrides:
                  config.model = global_overrides["force_model"]

          # Try preferred provider first
          preferred_provider = self.providers.get(config.provider)
          if preferred_provider and await preferred_provider.check_availability():
              logger.info(f"Using preferred provider: {config.provider}")
              return preferred_provider

          # Fallback to available providers
          for provider_name in self.fallback_order:
              if provider_name == config.provider:
                  continue  # Already tried

              fallback_provider = self.providers.get(provider_name)
              if fallback_provider and await fallback_provider.check_availability():
                  logger.warning(
                      f"Falling back from {config.provider} to {provider_name}"
                  )
                  config.provider = provider_name
                  return fallback_provider

          raise LLMProviderError("No available LLM providers")

  üìä Profile-Based Processing Engine

  Profile Definition Standards

  from typing import List, Dict, Any, Optional
  from pydantic import BaseModel, Field, validator

  class ProcessingStep(BaseModel):
      """Single step in a multi-step processing profile."""
      step_id: str = Field(..., min_length=1, max_length=50)
      name: str = Field(..., min_length=1, max_length=100)
      description: Optional[str] = None
      prompt_template: str = Field(..., min_length=10)
      model_config: ModelConfig = Field(default_factory=ModelConfig)
      output_format: str = Field(default="json", pattern="^(json|text)$")
      required: bool = True
      pass_to_next: bool = True

      @validator('prompt_template')
      def validate_prompt_template(cls, v):
          """Ensure prompt template has required placeholders."""
          if '{transcript}' not in v:
              raise ValueError("Prompt template must include {transcript} placeholder")
          return v

  class ProcessingProfile(BaseModel):
      """Complete profile definition for transcript analysis."""
      profile_id: str = Field(..., pattern="^[a-z0-9_]+$")
      name: str = Field(..., min_length=1, max_length=100)
      description: str = Field(..., min_length=1, max_length=500)
      version: str = Field(default="1.0.0", pattern=r"^\d+\.\d+\.\d+$")
      steps: List[ProcessingStep] = Field(..., min_items=1, max_items=10)
      final_output_schema: Dict[str, Any] = Field(default_factory=dict)
      tags: List[str] = Field(default_factory=list)
      use_cases: List[str] = Field(default_factory=list)
      estimated_tokens: int = Field(default=1000, ge=100, le=50000)

      @validator('steps')
      def validate_steps_unique_ids(cls, v):
          """Ensure all step IDs are unique."""
          step_ids = [step.step_id for step in v]
          if len(step_ids) != len(set(step_ids)):
              raise ValueError("All step IDs must be unique")
          return v

  # Built-in profile examples
  BUSINESS_MEETING_PROFILE = ProcessingProfile(
      profile_id="business_meeting",
      name="Business Meeting Analysis",
      description="Extract entities, decisions, and action items from business meetings",
      steps=[
          ProcessingStep(
              step_id="extract_entities",
              name="Extract Key Entities",
              prompt_template="""
              Extract key entities from this meeting transcript:
              
              {transcript}
              
              Return JSON with:
              - people: list of person names mentioned
              - companies: list of company/organization names
              - dates: list of dates mentioned
              - locations: list of locations mentioned
              """,
              model_config=ModelConfig(provider="local", model="llama3.1:8b", temperature=0.1)
          ),
          ProcessingStep(
              step_id="analyze_decisions",
              name="Analyze Decisions Made",
              prompt_template="""
              Based on this transcript and the entities extracted:
              
              Transcript: {transcript}
              Entities: {extract_entities}
              
              Identify decisions made in the meeting. Return JSON with:
              - decisions: list of decisions with responsible person and deadline
              - next_steps: concrete action items identified
              """,
              model_config=ModelConfig(provider="openrouter", model="openai/gpt-4o-mini", temperature=0.2)
          )
      ],
      estimated_tokens=4500
  )

  Profile Execution Engine

  import json
  from typing import Dict, Any, Optional
  import asyncio

  class ProfileExecutor:
      """Execute multi-step processing profiles."""

      def __init__(self, model_selector: ModelSelector):
          self.model_selector = model_selector
          self.logger = logging.getLogger(__name__)

      async def execute_profile(
          self,
          profile: ProcessingProfile,
          transcript: str,
          global_overrides: Optional[Dict[str, Any]] = None
      ) -> Dict[str, Any]:
          """Execute complete processing profile."""

          execution_context = {
              "transcript": transcript,
              "profile_id": profile.profile_id,
              "steps_completed": 0,
              "total_tokens": 0,
              "start_time": asyncio.get_event_loop().time()
          }

          step_outputs = {}

          try:
              for step in profile.steps:
                  self.logger.info(f"Executing step: {step.step_id}")

                  # Apply global overrides to step config
                  step_config = step.model_config.copy()
                  if global_overrides:
                      step_overrides = global_overrides.get("step_overrides", {}).get(step.step_id, {})
                      for key, value in step_overrides.items():
                          setattr(step_config, key, value)

                  # Execute step
                  try:
                      step_result = await self._execute_step(
                          step, step_outputs, execution_context, step_config, global_overrides
                      )

                      if step.pass_to_next:
                          step_outputs[step.step_id] = step_result["content"]

                      execution_context["steps_completed"] += 1
                      execution_context["total_tokens"] += step_result.get("tokens_used", 0)

                  except Exception as e:
                      if step.required:
                          raise ProcessingError(f"Required step {step.step_id} failed: {e}")
                      else:
                          self.logger.warning(f"Optional step {step.step_id} failed: {e}")
                          continue

              execution_time = asyncio.get_event_loop().time() - execution_context["start_time"]

              return {
                  "result": step_outputs,
                  "metadata": {
                      "profile_id": profile.profile_id,
                      "steps_completed": execution_context["steps_completed"],
                      "total_tokens": execution_context["total_tokens"],
                      "processing_time_ms": int(execution_time * 1000)
                  }
              }

          except Exception as e:
              self.logger.exception(f"Profile execution failed: {e}")
              raise ProcessingError(f"Profile execution failed: {e}")

      async def _execute_step(
          self,
          step: ProcessingStep,
          step_outputs: Dict[str, Any],
          context: Dict[str, Any],
          config: ModelConfig,
          global_overrides: Optional[Dict[str, Any]] = None
      ) -> Dict[str, Any]:
          """Execute a single processing step."""

          # Build prompt with variable substitution
          prompt_vars = {
              "transcript": context["transcript"],
              **step_outputs  # Previous step outputs available
          }

          prompt = step.prompt_template.format(**prompt_vars)

          # Get provider and execute
          provider = await self.model_selector.select_provider(config, global_overrides)
          result = await provider.generate_completion(prompt, config)

          # Validate JSON output if expected
          if step.output_format == "json":
              try:
                  parsed = json.loads(result["content"])
                  result["content"] = parsed
              except json.JSONDecodeError as e:
                  raise ProcessingError(f"Step {step.step_id} returned invalid JSON: {e}")

          return result

  üöÄ FastAPI Microservice Integration

  Jubal Service Contract Implementation

  from fastapi import FastAPI, HTTPException, status, Depends
  from fastapi.middleware.cors import CORSMiddleware
  import redis.asyncio as redis
  from typing import Dict, Any

  # Jubal envelope models
  class JubalEnvelope(BaseModel):
      """Standard Jubal data envelope format."""
      job_id: str
      pipeline_id: Optional[str] = None
      data: Dict[str, Any]
      metadata: Dict[str, Any] = Field(default_factory=dict)
      trace: Dict[str, Any] = Field(default_factory=dict)

  class JubalResponse(BaseModel):
      """Standard Jubal response format."""
      job_id: str
      status: str = Field(pattern="^(completed|error|processing)$")
      data: Optional[Dict[str, Any]] = None
      error: Optional[Dict[str, Any]] = None
      metadata: Dict[str, Any] = Field(default_factory=dict)

  # FastAPI application setup
  app = FastAPI(
      title="Grok Intelligence Engine",
      description="AI-powered transcript analysis microservice for the Jubal ecosystem",
      version="1.0.0",
      docs_url="/docs",
      redoc_url="/redoc"
  )

  app.add_middleware(
      CORSMiddleware,
      allow_origins=["*"],  # Configure appropriately for production
      allow_credentials=True,
      allow_methods=["*"],
      allow_headers=["*"],
  )

  # Service registry integration
  class ServiceRegistry:
      """Redis-based service registry integration."""

      def __init__(self, redis_url: str):
          self.redis_url = redis_url
          self.redis_client: Optional[redis.Redis] = None
          self.service_info = {
              "service": "grok-adapter",
              "version": "1.0.0",
              "host": "grok-adapter",
              "port": 8002,
              "health_endpoint": "/health",
              "capabilities": ["analyze", "extract", "summarize"]
          }

      async def connect(self):
          """Connect to Redis service registry."""
          self.redis_client = redis.from_url(self.redis_url)
          await self.register_service()

      async def register_service(self):
          """Register this service with the registry."""
          if self.redis_client:
              await self.redis_client.hset(
                  "jubal:services",
                  "grok-adapter",
                  json.dumps(self.service_info)
              )

  # Dependency injection
  async def get_profile_executor() -> ProfileExecutor:
      """Get configured profile executor."""
      # Initialize providers
      ollama_provider = OllamaProvider()
      openrouter_provider = OpenRouterProvider(api_key=settings.openrouter_api_key)

      providers = {
          "local": ollama_provider,
          "openrouter": openrouter_provider
      }

      model_selector = ModelSelector(providers)
      return ProfileExecutor(model_selector)

  # Core endpoints
  @app.get("/health")
  async def health_check():
      """Jubal-required health check endpoint."""
      return {
          "status": "healthy",
          "service": "grok-adapter",
          "version": "1.0.0",
          "timestamp": datetime.now(UTC).isoformat()
      }

  @app.get("/capabilities")
  async def get_capabilities():
      """Jubal-required capabilities declaration."""
      return {
          "service": "grok-adapter",
          "version": "1.0.0",
          "accepts": ["text/plain"],
          "provides": ["application/json"],
          "operations": ["analyze", "extract", "summarize"],
          "features": {
              "multi_step_processing": True,
              "profile_based_analysis": True,
              "model_selection": True,
              "global_overrides": True
          },
          "supported_providers": ["local", "openrouter"],
          "available_profiles": ["business_meeting", "project_planning", "personal_notes"]
      }

  @app.post("/process", response_model=JubalResponse)
  async def process_transcript(
      envelope: JubalEnvelope,
      executor: ProfileExecutor = Depends(get_profile_executor)
  ):
      """Main processing endpoint following Jubal contract."""

      try:
          # Validate input
          if envelope.data.get("type") != "text/plain":
              raise HTTPException(
                  status_code=status.HTTP_400_BAD_REQUEST,
                  detail="Only text/plain content type supported"
              )

          transcript = envelope.data.get("content", "")
          if not transcript.strip():
              raise HTTPException(
                  status_code=status.HTTP_400_BAD_REQUEST,
                  detail="Empty transcript content"
              )

          # Get profile
          profile_id = envelope.metadata.get("profile_id", "business_meeting")
          profile = await load_profile(profile_id)

          # Apply overrides
          overrides = envelope.metadata.get("overrides", {})

          # Execute processing
          result = await executor.execute_profile(profile, transcript, overrides)

          return JubalResponse(
              job_id=envelope.job_id,
              status="completed",
              data={
                  "type": "application/json",
                  "content": result["result"],
                  "encoding": "utf-8"
              },
              metadata={
                  **envelope.metadata,
                  **result["metadata"],
                  "timestamp": datetime.now(UTC).isoformat()
              }
          )

      except ProcessingError as e:
          logger.exception(f"Processing error for job {envelope.job_id}: {e}")
          return JubalResponse(
              job_id=envelope.job_id,
              status="error",
              error={
                  "code": "PROCESSING_ERROR",
                  "message": str(e),
                  "recoverable": True
              }
          )

      except Exception as e:
          logger.exception(f"Unexpected error for job {envelope.job_id}: {e}")
          return JubalResponse(
              job_id=envelope.job_id,
              status="error",
              error={
                  "code": "INTERNAL_ERROR",
                  "message": "Internal processing error",
                  "recoverable": False
              }
          )

  üê≥ Docker & Container Optimization

  Multi-Stage Dockerfile for Production

  # Dockerfile
  FROM python:3.12-slim as builder

  # Install UV for fast dependency management
  COPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv

  # Set working directory
  WORKDIR /app

  # Copy dependency files
  COPY pyproject.toml uv.lock ./

  # Install dependencies in virtual environment
  RUN uv sync --frozen --no-dev

  # Production stage
  FROM python:3.12-slim as production

  # Install runtime dependencies
  RUN apt-get update && apt-get install -y \
      curl \
      && rm -rf /var/lib/apt/lists/*

  # Copy virtual environment from builder
  COPY --from=builder /app/.venv /app/.venv

  # Set PATH to use virtual environment
  ENV PATH="/app/.venv/bin:$PATH"

  # Create non-root user
  RUN groupadd -r grok && useradd -r -g grok grok

  # Set working directory
  WORKDIR /app

  # Copy application code
  COPY --chown=grok:grok . .

  # Switch to non-root user
  USER grok

  # Health check
  HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
      CMD curl -f http://localhost:8002/health || exit 1

  # Expose port
  EXPOSE 8002

  # Run application
  CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8002"]

  Docker Compose for Development

  # docker-compose.yml
  version: '3.8'

  services:
    grok-adapter:
      build: .
      container_name: jubal-grok
      ports:
        - "8002:8002"
      environment:
        - GROK_REDIS_URL=redis://redis:6379/0
        - GROK_OLLAMA_URL=http://host.docker.internal:11434
        - GROK_OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
        - GROK_DEBUG=true
      volumes:
        - ./profiles:/app/profiles
        - ./logs:/app/logs
      depends_on:
        - redis
      networks:
        - jubal-network
      restart: unless-stopped

    redis:
      image: redis:7-alpine
      container_name: jubal-redis
      ports:
        - "6379:6379"
      volumes:
        - redis_data:/data
      networks:
        - jubal-network
      restart: unless-stopped

    postgres:
      image: postgres:15-alpine
      container_name: jubal-postgres
      environment:
        - POSTGRES_DB=jubal_dev
        - POSTGRES_USER=jubal
        - POSTGRES_PASSWORD=jubal_dev_password
      ports:
        - "5432:5432"
      volumes:
        - postgres_data:/var/lib/postgresql/data
      networks:
        - jubal-network
      restart: unless-stopped

  volumes:
    redis_data:
    postgres_data:

  networks:
    jubal-network:
      driver: bridge

  üß™ AI-Specific Testing Strategies

  Testing LLM Integration

  import pytest
  from unittest.mock import AsyncMock, patch
  import json

  class TestLLMProviders:
      """Test LLM provider integrations with mocked responses."""

      @pytest.fixture
      def mock_ollama_response(self):
          """Mock successful Ollama response."""
          return {
              "response": '{"entities": {"people": ["John Doe"], "companies": ["Acme Corp"]}}',
              "eval_count": 150
          }

      @pytest.fixture
      def sample_transcript(self):
          """Sample meeting transcript for testing."""
          return """
          John: Welcome everyone to today's meeting about the Acme Corp integration.
          Jane: Thanks John. I think we should focus on the API requirements first.
          John: Agreed. Let's target completion by next Friday.
          """

      @pytest.mark.asyncio
      async def test_ollama_provider_success(self, mock_ollama_response):
          """Test successful Ollama provider response."""
          with patch('aiohttp.ClientSession.post') as mock_post:
              # Setup mock response
              mock_response = AsyncMock()
              mock_response.status = 200
              mock_response.json.return_value = mock_ollama_response
              mock_post.return_value.__aenter__.return_value = mock_response

              # Test provider
              provider = OllamaProvider()
              config = ModelConfig(provider="local", model="llama3.1:8b")

              result = await provider.generate_completion(
                  "Extract entities from: {sample_transcript}",
                  config
              )

              assert result["content"] == mock_ollama_response["response"]
              assert result["tokens_used"] == 150
              assert result["provider"] == "ollama"

      @pytest.mark.asyncio
      async def test_provider_fallback(self):
          """Test fallback from local to cloud provider."""
          # Mock local provider failure
          mock_local = AsyncMock()
          mock_local.check_availability.return_value = False

          # Mock cloud provider success
          mock_cloud = AsyncMock()
          mock_cloud.check_availability.return_value = True

          providers = {"local": mock_local, "openrouter": mock_cloud}
          selector = ModelSelector(providers)

          config = ModelConfig(provider="local")
          selected = await selector.select_provider(config)

          assert selected == mock_cloud
          assert config.provider == "openrouter"

  class TestProfileExecution:
      """Test profile execution with mocked LLM responses."""

      @pytest.fixture
      def business_meeting_profile(self):
          """Sample business meeting profile."""
          return BUSINESS_MEETING_PROFILE

      @pytest.fixture
      def mock_executor(self):
          """Mock profile executor with controlled responses."""
          executor = AsyncMock(spec=ProfileExecutor)
          return executor

      @pytest.mark.asyncio
      async def test_complete_profile_execution(
          self, 
          business_meeting_profile, 
          sample_transcript,
          mock_executor
      ):
          """Test complete profile execution flow."""
          # Mock execution result
          expected_result = {
              "result": {
                  "extract_entities": {
                      "people": ["John", "Jane"],
                      "companies": ["Acme Corp"]
                  },
                  "analyze_decisions": {
                      "decisions": [
                          {
                              "decision": "Focus on API requirements",
                              "responsible": "Jane",
                              "deadline": "next Friday"
                          }
                      ]
                  }
              },
              "metadata": {
                  "profile_id": "business_meeting",
                  "steps_completed": 2,
                  "total_tokens": 300,
                  "processing_time_ms": 2000
              }
          }

          mock_executor.execute_profile.return_value = expected_result

          result = await mock_executor.execute_profile(
              business_meeting_profile,
              sample_transcript
          )

          assert result["result"]["extract_entities"]["people"] == ["John", "Jane"]
          assert result["metadata"]["steps_completed"] == 2
          assert result["metadata"]["total_tokens"] == 300

  @pytest.mark.integration
  class TestEndToEndProcessing:
      """Integration tests for complete processing pipeline."""

      @pytest.mark.asyncio
      async def test_process_endpoint_integration(self, client, sample_transcript):
          """Test complete /process endpoint with real profile."""
          envelope = {
              "job_id": "test-job-123",
              "data": {
                  "type": "text/plain",
                  "content": sample_transcript,
                  "encoding": "utf-8"
              },
              "metadata": {
                  "profile_id": "business_meeting"
              }
          }

          response = await client.post("/process", json=envelope)

          assert response.status_code == 200
          result = response.json()

          assert result["job_id"] == "test-job-123"
          assert result["status"] == "completed"
          assert "entities" in result["data"]["content"]
          assert "decisions" in result["data"]["content"]

  üìà Performance Monitoring for AI Services

  Custom Metrics and Monitoring

  import time
  from functools import wraps
  from typing import Dict, Any
  import prometheus_client

  # Prometheus metrics
  REQUEST_COUNT = prometheus_client.Counter(
      'grok_requests_total',
      'Total requests processed',
      ['profile_id', 'provider', 'status']
  )

  REQUEST_DURATION = prometheus_client.Histogram(
      'grok_request_duration_seconds',
      'Request duration in seconds',
      ['profile_id', 'provider']
  )

  TOKEN_USAGE = prometheus_client.Counter(
      'grok_tokens_total',
      'Total tokens processed',
      ['provider', 'model']
  )

  def monitor_llm_performance(func):
      """Decorator to monitor LLM provider performance."""
      @wraps(func)
      async def wrapper(*args, **kwargs):
          start_time = time.time()
          provider = kwargs.get('config', {}).provider if 'config' in kwargs else 'unknown'

          try:
              result = await func(*args, **kwargs)

              # Record metrics
              duration = time.time() - start_time
              REQUEST_DURATION.labels(
                  profile_id='unknown',
                  provider=provider
              ).observe(duration)

              if 'tokens_used' in result:
                  TOKEN_USAGE.labels(
                      provider=provider,
                      model=kwargs.get('config', {}).model if 'config' in kwargs else 'unknown'
                  ).inc(result['tokens_used'])

              REQUEST_COUNT.labels(
                  profile_id='unknown',
                  provider=provider,
                  status='success'
              ).inc()

              return result

          except Exception as e:
              REQUEST_COUNT.labels(
                  profile_id='unknown',
                  provider=provider,
                  status='error'
              ).inc()
              raise

      return wrapper

  # Add to FastAPI app
  @app.get("/metrics")
  async def get_metrics():
      """Prometheus metrics endpoint."""
      return Response(
          prometheus_client.generate_latest(),
          media_type="text/plain"
      )

  üõ°Ô∏è AI-Specific Security Considerations

  Prompt Injection Prevention

  import re
  from typing import List

  class PromptSanitizer:
      """Sanitize and validate prompts to prevent injection attacks."""

      DANGEROUS_PATTERNS = [
          r"ignore\s+previous\s+instructions",
          r"system\s*:",
          r"<\s*script\s*>",
          r"javascript\s*:",
          r"data\s*:\s*text/html",
          r"eval\s*\(",
          r"exec\s*\(",
      ]

      @classmethod
      def sanitize_prompt(cls, prompt: str) -> str:
          """Sanitize prompt content."""
          # Remove potentially dangerous patterns
          sanitized = prompt
          for pattern in cls.DANGEROUS_PATTERNS:
              sanitized = re.sub(pattern, "[REDACTED]", sanitized, flags=re.IGNORECASE)

          # Limit prompt length
          if len(sanitized) > 50000:  # 50k character limit
              sanitized = sanitized[:50000] + "...[TRUNCATED]"

          return sanitized

      @classmethod
      def validate_template(cls, template: str) -> bool:
          """Validate prompt template safety."""
          # Check for dangerous patterns
          for pattern in cls.DANGEROUS_PATTERNS:
              if re.search(pattern, template, flags=re.IGNORECASE):
                  return False

          # Ensure required placeholders exist
          if '{transcript}' not in template:
              return False

          return True

  # Apply to prompt processing
  async def safe_prompt_execution(prompt_template: str, variables: Dict[str, Any]) -> str:
      """Safely execute prompt with sanitization."""

      # Validate template
      if not PromptSanitizer.validate_template(prompt_template):
          raise SecurityError("Unsafe prompt template detected")

      # Sanitize variables
      sanitized_vars = {}
      for key, value in variables.items():
          if isinstance(value, str):
              sanitized_vars[key] = PromptSanitizer.sanitize_prompt(value)
          else:
              sanitized_vars[key] = value

      # Build final prompt
      final_prompt = prompt_template.format(**sanitized_vars)

      # Final sanitization check
      return PromptSanitizer.sanitize_prompt(final_prompt)

  üöÄ Development Commands for AI Stack

  # Start local LLM service (Ollama)
  ollama serve
  ollama pull llama3.1:8b
  ollama pull mistral:7b

  # Test LLM connectivity
  curl http://localhost:11434/api/tags

  # Run Grok service locally
  uv run uvicorn app.main:app --reload --host 0.0.0.0 --port 8002

  # Test profile execution
  curl -X POST http://localhost:8002/process \
    -H "Content-Type: application/json" \
    -d @test_data/business_meeting_sample.json

  # Monitor performance
  curl http://localhost:8002/metrics

  # Load test with multiple profiles
  uv run python scripts/load_test.py --profiles business_meeting,project_planning

  # Profile validation
  uv run python scripts/validate_profiles.py --profile-dir ./profiles

  # Token usage analysis
  uv run python scripts/analyze_token_usage.py --log-file ./logs/grok.log